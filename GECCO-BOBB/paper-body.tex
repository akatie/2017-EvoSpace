\section{Introduction}
Asynchronous EAs \cite{Jini:FEA2000,alba2001analyzing,Jini:FEA2000,jj:2008:PPSN} have
started to become common only relatively recently, in an effort to 
exploit computing resources available through different Internet technologies,
including cloud and volunteer-based. In this work, we are
interested in benchmarking asynchronous EAs following a pool-based 
approach, we will refer to such algorithms as {\em Pool-based EAs} 
or PEAs, and highlight the fact that such systems are 
intrinsically parallel, distributed and asynchronous.

Pool-EAs differ from the closely related Island Model, 
mainly with regards to the responsibilities assigned to 
the server. When there is a server in the island model, it is 
responsible for the interaction and synchronization of 
all the populations.  In Pool-EAs, on the other hand, the population repository only 
receives stateless requests from isolated workers 
or clients. In this way, Pool-EAs are capable of using and leveraging an 
ad-hoc and ephemeral collaboration of computing resources. 

The algorithm presented in this paper follows the basic 
EvoSpace model \cite{GValdez2015} in which EvoWorkers 
asynchronously interact with the population pool by 
taking samples of the population to perform a local 
evolutionary search on the samples, to then return newly
evolved solutions back to the pool. This is a
particular instance of a Pool-based EA, which, as long as there is a
common population pool, leaves every other detail to particular
implementations. 

To test this PEA against the Noiseless BBOB testbed, 
we mixed two canonical versions of the GA and PSO algorithms, 
by just defining EvoWorkers of each kind. The performance reported 
although not achieving an outstanding performance when compared 
against the leading algorithms, the results obtained highlight 
that a Pool Based approach can be used to improve the performance 
of single population-based optimization algorithms.


\subsection{PSO and GA Workers}
\label{sec:evoworkers}
As we mentioned earlier, EvoWorkers are independent of 
the population repository, and developers can implement them 
in any language that supports HTTP requests. In this work,  
EvoWorkers were implemented in Python taking advantage of 
two open source libraries of nature inspired optimization 
metaheuristics:  DEAP \cite{fortin2012deap} and EvoloPy 
\cite{faris2016evolopy}. For each algorithm a 
python script was responsible for the initialization using 
the required parameters and setting up the initial population, 
then after some iterations, the current population and 
benchmark data is sent back to the server. The source code 
for the Python EvoWorkers proposed in this work are in 
the following GitHub repository: \url{https://github.com/mariosky/EvoWorker}.

\section{Experiment}
 \label{sec:experiments}

A script was responsible for creating the EvoWorker 
containers and running the testbed. The maximum number of 
function evaluations was set to $10^5*dim$. 
In order to maintain the required number of function 
evaluations the following EvoWorker setup was set for
each dimension (see Table~\ref{tab:params}).

\begin{table}
  \small
  \caption{EvoWorker Setup}
  \label{tab:params} 
  \centering
  \small
  \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline
    Dimension & 2 & 3 & 5 & 10 & 20 & 40\\ \hline
    Iterations per Sample  & 50 & 50 & 50 & 50 & 50 & 50\\ \hline
    Sample Size  & 100 & 100 & 100 & 200 & 200 & 200 \\ \hline
    Samples per Worker & 20 & 30 & 25 & 25 & 25 & 25  \\ \hline
    PSO Workers & 1 & 1 & 2 & 2 & 4 & 8  \\ \hline
  \end{tabular}
\end{table}

The parameters for each type of EvoWorker shown in Table\ref{tab:GAparams}
and Table\ref{tab:GAparams} for GAs and PSO respectively.
\begin{table}
  \small
  \caption{ DEAP GA EvoWorker Parameters }
  \label{tab:GAparams} 
  \centering
  \small
  \begin{tabular}{|l|c|}
    \hline
    Search space &  $[-4,4]^{D}$ \\ \hline
    Selection & Tournament size=12\\ \hline
    Mutation & Gaussian $\mu=0.0$, $\sigma=0.5$, indbp=0.05  \\ \hline
    Mutation Probability & [.1,.6]  \\ \hline
    Crossover & Two Point  \\ \hline
    Crossover Probability& [.8,1]  \\ \hline
  \end{tabular}
\end{table}

\begin{table}
  \small
  \caption{ EvoloPy PSO EvoWorker Parameters }
  \label{tab:GAparams} 
  \centering
  \small
  \begin{tabular}{|l|c|}
    \hline
    Search space &  $[-4,4]^{D}$ \\ \hline
    $V_{max}$ & 6 \\ \hline
    $W_{max}$ & $0.9$ \\ \hline
    $W_{min}$ & $0.2$ \\ \hline
    $C_1$ & 2 \\ \hline
    $C_2$ & 2 \\ \hline
  \end{tabular}
\end{table}

After the  execution, a script processed the logs and 
generated the files needed by the COCO platform post-processing scripts. 

A requirement of the COCO platform is that it needs 
to inspect each function evaluation to keep the log required 
to analyze the execution. The logging code maintains 
a sequential record of the number function calls. 
This exact order is not practical to keep in an asynchronous 
execution as many workers are calling the function at the 
same time. For this reason, the granularity of the number of 
function evaluations and their order is kept at the 
sample-iteration level. As we mentioned earlier, each worker 
returns a record with benchmark data, with the number of 
evaluations performed in each iteration.

Is not practical to track the exact sequential 
number of function evaluations in an asynchronous 
execution, because many workers could be calling 
the function at the same time. For this reason, the 
granularity of the number of function evaluations and 
their order was kept at the sample and iteration level. 
As we mentioned earlier, each worker returns the number 
of evaluations performed in each iteration. The order 
of function calls was given by the order in which the 
server received the samples and the order of the 
iterations in each. On the other hand, the number of 
function evaluations is incremented in each iteration 
by the sample size and the best function evaluation is 
assigned that number,  as if in each iteration the best 
solution was found in the last function evaluation. Instead 
of incrementing the number by one it is incremented by the 
number of solutions in the sample. 
Is important to notice that EvoWorkers run the algorithm 
only for a small number of iterations and with 
a relatively small sample of the population. For instance,
for the COCO benchmark presented in the case study the maximum number of 
function evaluations in a single iteration was 200.

The COCO post-precessing script currently has an assetion stating
that all the function evaluations start at number 1. In our
case the first evalauation had a number equal to the total number
of evalautions in a single iteration. In order to run the script 
a dummy function evaluation with number one was inserted first. 

\subsection{Results}
\label{sec:results}

After the experiment was executed a script generated 
the files and folders needed by the COCO post-processing 
scripts; this script generated the comparative tables and
graphics for checking the performance of the algorithm against
those found in the COCO repository. Results from experiments 
according to \cite{hansen2016coco} on the benchmark functions 
are presented in Figures.
The results obtained show that the 
algorithm performs quite well on separable functions, 
both regarding results and scaling when compared to results 
from other nature inspired algorithms \cite{hansen2010bbob}.

\section{Discussion}
\label{sec:siscussion}

The results obtained show that an asynchronous execution 
following a Pool-based approach is  possible and easy to achieve.
Results, however, are still preliminary and further tuning of the parameters could 
potentially wield better results.

Future lines of work will focus on using other EA or 
meta-heuristic techniques, such as the Grey Wolf Optimizer \cite{mirjalili2014grey}
or Differential Evolution \cite{storn1997differential} for having workers that are 
heterogeneous in more than one sense. RPSS could be used 
in those cases where each algorithm has a different set of 
parameters, but also to randomly select the technique employed 
in each node.

\begin{acks}
  Anonymized for double-blind review
%This work has been supported in part by:  Ministerio espa\~{n}ol de
%Econom\'{\i}a y Competitividad under project TIN2014-56494-C4-3-P
%(UGR-EPHEMECH).
\end{acks}
