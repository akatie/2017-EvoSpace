\section{Introduction}
% Main Ideas:
%Why Paralel
A large body of work exists on the parallelization of EAs,
with techniques leveraging multiple CPU cores, many computing nodes, 
and GPUs \cite{muhlenbein1989parallel,cantu2000efficient,hofmann2013performance}. 
However, asynchronous EAs
\cite{Jini:FEA2000,alba2001analyzing,Jini:FEA2000,jj:2008:PPSN} have
started to become common only relatively
recently, in an effort to exploit computing resources available
through different Internet technologies, including cloud. In this work, we are
interested in those asynchronous EAs following a pool-based approach,
where a collection of heterogeneous worker processes 
carry out population search tasks by collaborating through a shared
individual 
repository or population pool. We will refer to such algorithms 
as {\em Pool-based EAs} or PEAs, and highlight the fact that 
such systems are intrinsically parallel, distributed and asynchronous.

Pool-EAs differ from the closely related Island Model, 
mainly with regards to the responsibilities assigned to 
the server. When there is a server in the island model, it is 
responsible for the interaction and synchronization of 
all the populations.  In Pool-EAs, on the other hand, the population repository only 
receives stateless requests from isolated workers 
or clients. In this way, Pool-EAs are capable of using and leveraging an 
ad-hoc and ephemeral collaboration of computing resources. 

The platform presented in this paper is a new implementation 
of the EvoSpace model \cite{GValdez2015} in which 
workers asynchronously interact with the population 
pool by taking samples of the population 
to perform a local evolutionary search on the samples, 
to then return newly evolved solutions back to the pool. This is a
particular instance of a Pool-based EA, which, as long as there is a
common population pool, leaves every other detail to particular
implementations. Other PEAs, for instance, might return only one
individual to the pool, or use it as a read-only resource, not
returning any member to it; the {\em frequency} with which resources
are taken or returned to the population are not set by the model
either, leaving it as a implementation or model-specific parameter. % I am giving here a general overview of PEAs, maybe it should be moved up with a few references? - JJ

The previous version % anonymized reference
was implemented using CherryPy, a basic HTTP 
server written in Python. This new version uses Node.js, an 
event-driven interpreter with a built-in event loop capable of
asynchronous I/O \cite{tilkov2010node}, that is 
running on the JavaSacript V8 engine. Node.js is used 
to optimize throughput and scalability of the server. %Actually you
                                %are using express, I think. You
                                %should talk about that - JJ
                                % Ok, I will later when explaining the server 
                                % in detail - Mario 

Additionaly to the  increased performance this version 
adds new functionality: In the former version workers could only
ask for random samples of a particular size, now clients 
can retrieve objects from the server ordered by a score. 
Designers can use this functionality to implement 
asynchronous versions of the island model or to force 
the retrieval of different objects in every request 
resembling a circular queue. Instead of using the JSON-RPC 
protocol the server functionality is now exposed as a RESTful 
Web Service. The server now keeps a log of the work performed 
by workers: The number of evaluations, the best solution in each 
generation (or iteration), parameters and algorithm used among others.
This log can later be used to compare the performance of 
the algorithm against others, for instance against 
algorithms using the COCO (COmparing Continuous Optimisers)
platform \cite{hansen2016coco}.
The aim of the {\sf evospace-js} software is to provide 
researchers with a high performance platform in which 
they can execute pool-based algorithms using heterogeneous workers. 

The remainder of the paper proceeds as follows. Section \ref{sec:work} 
reviews related work. Afterwards, Section \ref{sec:evo} describes the
proposed EvoSpace implementation, the experimental work is presented in 
Section \ref{sec:experiments}. Finally, a summary and 
concluding remarks are given in Section \ref{sec:conclusions}.


\section{Related Work}
\label{sec:work}
There are two important practical issues faced by many EA and other
optimization systems, namely the size of the parameter 
space and the high computational cost when it is compared with
mathematical programming or numerical techniques. % I guess because
% they are deterministic, right? - JJ
% Yes - Mario
Concerning the latter, one approach to mitigate this issue is to use parallel or 
distributed implementations \cite{cantu-paz:migration-policies,duda2013gpu}.
For instance, Fern\'andez et al. \cite{nc} % articulo Paco, Gustavo y Leo publucado en Natural Computing}
use the well-known Berkeley Open Infrastructure for Network Computing (BOINC) to distribute EA runs across a
heterogeneous network of volunteer computers using virtual machines. Another recent example is 
found in the FlexGP system developed by Sherry et al. \cite{sherry2012flex}. FlexGP is probably the first large scale GP system 
that runs on the cloud, using an island model approach and implemented over Amazon EC2 with a 
socket-based client-server architecture. There is a considerable
improvement in performance and scalability in this approach, but this scalability has
a cost which is proportionally much smaller than installing a
permanent infrastructure, but onerous nonetheless. % Maybe take some
                                % figures from it - JJ

In general, all the techniques and implementations mentioned above
rely on more or less {\em traditional} parallel or distributed
evolutionary algorithms, using {\em farming} for offloading
evaluations to ephemeral resources or using more traditional
island-based models in distributed or cloud-based resources. However,
there is another approach to distributed EAs: the so called Pool-based
architecture \cite{talukdar1998asynchronous,pool:ga,de1991genetic}. In general, a 
Pool-based system employs a central repository 
% What's the difference? - JJ
% Is central but it could be internally distributed or a cluster
% Im taking it of because is an implemantation detail - M 
where the evolving population, or a part of it, is stored. 
Distributed clients interact with the pool, performing some or all of the basic EA processes 
(selection, genetic operators, survival), but these clients join the
search by just using an API, and quit by simply not doing it any
more. Clients are not considered reliable in any way, and the
threshold to join the pool and perform an operation is kept as low as
possible. 
A representative work of this approach 
is that by Merelo et al. \cite{agajaj} implementing a JavaScript based PEA that distributes 
the evolutionary process over the web, providing the added advantage of not requiring the 
installation of additional software in each computing node.  Other similar cloud-based solutions 
are based on a global queue of tasks and a Map-Reduce implementation which normally handles failures 
by the re-execution of  tasks \cite{fazenda2012,di2013towards,FlexGP}. Using the BOINC 
volunteer platform  Smaoui et al. \cite{FekiNG09} uses work units that consist of a fitness 
evaluation task and multiple replicas  were produced and sent to different clients.

While using a distributed framework can ease the computational cost, 
it can also exacerbate the first issue mentioned above;
i.e., it increases the size of the algorithm parameter space, which makes parameter tuning a more difficult task.
The issue of optimal parametrization of EAs is a widely studied subject \cite{de2007parameter}, 
with many approaches in literature. For instance, one of the most successful approaches 
is the F-Racing and iterative F-Racing techniques \cite{lopez2011irace}. 
However, while such algorithms can find high performance parametrization, 
they require additional computational effort which can be too expensive in some applications
(even if they are more efficient than an exhaustive search).

\begin{figure}[!t]
    \centering
        \includegraphics[width=2.5in]{img/classes.png}
    \caption{ UML Class diagram of the Population and Individual classes.}
    \label{fig:classes}
\end{figure}
%
\begin{figure*}[!t]
    \centering
        \includegraphics[width=5.4in]{img/evospace-js.png}
    \caption{ Stack diagram of the {\sf evospace-js} framework components.}
    \label{fig:stack}
\end{figure*}

\section{{\sf evospace-js} Implementation}
\label{sec:evo}
The main components of the EvoSpace framework are: the {\sf evospace-js} 
population repository, remote clients called EvoWorkers.
Each of these components are defined in the following subsections.

\subsection{{\sf evospace-js} Population Repository} % Maybe qualify this, it's got the same
                               % name as above. Population container?
                               % ~ JJ
                               % I like container but we have been using repository in
                               % other pape.
 \label{sec:evospace}
The {\sf evospace-js} server provides a collection of REST methods  
to operate over a set of objects $ES$, which can be seen as the 
population. Multiple populations can be created and are 
distinguished by their name. Objects in each $ES$ 
can be selected, removed and replaced through the 
following endpoints:
\begin{enumerate}
    \item {\bf population\_name/initialize} 
    This is a {\tt POST} request used to create a new population.
    \item {\bf population\_name/individual} 
    This is a {\tt POST} request used to create and add a new object
    to a population. The object is defined in a JSON format, 
    and there is no restriction on its structure, only 
    the following properties are required: ``id'' this is an 
    integer and is generated if not present, ``fitness'' also defined 
    as a JSON object, the structure been specific to each application, 
    and finally a ``chromosome'' property again defined as
    a JavaScript object giving the internal representation of 
    the solution, by default a it defined as list of objects. 
    There is also an optional integer property called 
    ``score'' used when objects are going to be retieved in a certain order.
    \item {\bf population\_name/sample/n}
    This is a {\tt GET}  request used to take from the population a 
    sample of {\bf n} objects. These objects are removed from the 
    population and are no longer available
    to other requests until and only if they are put back. 
    Objects can be returned to the population 
    either by a {\tt PUT} sample request called from the same 
    client or by a Respawn request. The reason for 
    this is to avoid concurrently write conflicts and duplication of work.
    \item {\bf population\_name/sample}
    This is a {\tt POST} request used to put back a sample to the population.
    The new sample is sent in the request body as a JSON object. 
    If the client created new objects or 
    changed their original state, these objects replace the originals. 
    \item {\bf population\_name/respawn}
    This is a {\tt POST} request used to put back {\bf n} samples to their 
    original state. The number of samples is sent in the request body. 
\end{enumerate}
There are other secondary REST endpoints used to: select all objects in a 
population, select objects with scores with in a range, read the 
top {\tt n} objects according to a score and read the number of
objects currently on the population.      

The above methods were implemented first as JavaScript library 
with two classes: {\tt Individual} and {\tt Population} depicted 
in Figure~\ref{fig:classes} with calls to the Redis memory store 
through the {\bf ioredis} 
asynchronous library. In order to expose the library as a 
REST Web service endpoints were implemented using the Express HTTP framework. 
An optional dashboard type application, can be used to inspect 
the populations currently available on the server.


When a worker is putting back a sample, it can send 
an additional property called {\tt benchmark\_data }
to send supplementary information about the execution 
of the experiment.  This data can later be used to 
benchmark the performance of the algorithm. This 
data is again stored in redis as an ordered list, 
keeping a log for each experiment. Currently the JSON 
benchmark data structure contains the following details 
to later be used by the COCO platform: the algorithm identifier, 
parameters used, name , dimension, instance and optimal value 
of the function that is been optimized, worker and experiment 
identifiers and finally a list of details of each iteration 
or generation of the local execution. The details include:
the best solution and the function value, and the number of
function evaluations requiered. Depending on the application
other data could be recorded. The source code for the {\sf evospace-js}
server is in the following Github repository: 
\url{https://github.com/mariosky/evospace-js}.


\subsection{EvoWorkers}
\label{sec:evoworkers}
As we mentioned earlier, EvoWorkers are independent of 
the population store, and developers can implement them 
in any language that supports HTTP requests. To develop an 
EvoWorker, a programmer could just write the code needed to 
take a sample of the population and use this sample to 
replace the initial population of a local algorithm. 
Then after a certain number of iterations return the 
current population back to the server.

In this work, EvoWorkers were implemented in Python 
using two open source libraries of nature inspired optimization 
metaheuristics:  DEAP and EvoloPy. For each algorithm a 
python script was responsible for the initialization using 
the required parameters and setting up the initial population, 
then after some iterations, the current population and 
benchmark data is sent back to the server. EvoWorker 
scripts can run in Docker containers, by receiving 
the initial parameters as environment variables, and 
the script ends when it reaches a maximum number of samples.
The source code for the Python EvoWorkers
proposed in this work are in the following GitHub repository: 
\url{https://github.com/mariosky/EvoWorker}.

\section{Experiments}
 \label{sec:experiments}
As a case study, a simple hybrid algorithm consisting 
of PSO and GA EvoWorkers was used to run a benchmark that 
included the first three functions found in the COCO platform:  
Sphere (F1), Ellipsoid (F2), and Rastrigin (F3). The objective 
of the algorithm is not to be a competitive solution for 
the optimization benchmark, as the intention is to test 
the software functionality. After the 
execution, a script processed the logs and generated the files 
needed by the COCO platform post-processing scripts. 

A requirement of the COCO platform is that it needs 
to inspect each function evaluation to keep the log required 
to analyze the execution. The logging code maintains 
a sequential record of the number function calls. 
This exact order is not practical to keep in an asynchronous 
execution as many workers are calling the function at the 
same time. For this reason, the granularity of the number of 
function evaluations and their order is kept at the 
sample-iteration level. As we mentioned earlier, each worker 
returns a record with benchmark data, with the number of 
evaluations performed in each iteration.

Is not practical to track the exact sequential 
number of function evaluations in an asynchronous 
execution, because many workers could be calling 
the function at the same time. For this reason, the 
granularity of the number of function evaluations and 
their order was kept at the sample and iteration level. 
As we mentioned earlier, each worker returns the number 
of evaluations performed in each iteration. The order 
of function calls was given by the order in which the 
server received the samples and the order of the 
iterations in each. On the other hand, the number of 
function evaluations is incremented in each iteration 
by the sample size and the best function evaluation is 
assigned that number,  as if in each iteration the best 
solution was found in the last function evaluation. Instead 
of incrementing the number by one it is incremented by the 
number of solutions in the sample. 
Is important to notice that EvoWorkers run the algorithm 
only for a small number of iterations and with 
a relatively small sample of the population. For instance,
for the COCO benchmark presented in the case study the maximum number of 
function evaluations in a single iteration was 200. 
  

\subsection{Set-up}
\label{sec:evoworkers}

A script was responsible for creating the EvoWorker 
containers and running the benchmark. The first three functions 
F1 to F3 were tested with 15 instances for each of 
the dimensions: 2, 3, 5, 10, 20, and 40. 
The maximum number of function evaluations was set to $10^5*dim$. 
In order to maintain the required number of function 
evaluations the following EvoWorker setup was set for
each dimension.

\begin{table}
  \small
  \caption{EvoWorker Setup}
  \label{tab:params} 
  \centering
  \small
  \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline
    Dimension & 2 & 3 & 5 & 10 & 20 & 40\\ \hline
    Iterations per Sample  & 50 & 50 & 50 & 50 & 50 & 50\\ \hline
    Sample Size  & 100 & 100 & 100 & 200 & 200 & 200 \\ \hline
    Samples per Worker & 20 & 30 & 25 & 25 & 25 & 25  \\ \hline
    PSO Workers & 1 & 1 & 2 & 2 & 4 & 8  \\ \hline
  \end{tabular}
\end{table}



\begin{table}
  \small
  \caption{ DEAP GA EvoWorker Parameters }
  \label{tab:GAparams} 
  \centering
  \small
  \begin{tabular}{|l|c|}
    \hline
    Search space &  $[-4,4]^{D}$ \\ \hline
    Selection & Tournament size=12\\ \hline
    Mutation & Gaussian $\mu=0.0$, $\sigma=0.5$, indbp=0.05  \\ \hline
    Mutation Probability & [.1,.6]  \\ \hline
    Crossover & Two Point  \\ \hline
    Crossover Probability& [.8,1]  \\ \hline
  \end{tabular}
\end{table}

\begin{table}
  \small
  \caption{ EvoloPy PSO EvoWorker Parameters }
  \label{tab:GAparams} 
  \centering
  \small
  \begin{tabular}{|l|c|}
    \hline
    Search space &  $[-4,4]^{D}$ \\ \hline
    $V_{max}$ & 6 \\ \hline
    $W_{max}$ & $0.9$ \\ \hline
    $W_{min}$ & $0.2$ \\ \hline
    $C_1$ & 2 \\ \hline
    $C_2$ & 2 \\ \hline
  \end{tabular}
\end{table}


%TO DO: TABLE
%Data in https://docs.google.com/spreadsheets/d/123s_p6ABtyT2wdifxjuHVGuXjTZECmfzOddxUTydbI8/edit?usp=sharing



\subsection{Results}
\label{sec:results}
After the experiment was executed a script generated 
the files and folders needed by the COCO post-processing 
scripts. The script generated the comparative tables and
graphics for comparing the performance of the algorithm with
those found in the COCO repository. Examples of these figures
are presented in Figures \ref{fig:sphere} to \ref{fig:ellipsoid}.
Another Python script is responsible for exporting the additional 
data contained in the logs to a file using a JSON format. 
This file can later be used to analyze further the asynchronous 
performance of the algorithm.  For instance to compare the 
performance of each of the EvoWorkers.

\begin{figure*}[h!t]
    \centering
        \includegraphics[width=5in]{img/Sphere.pdf}
    \caption{Average numbers of function evaluations to reach target
      for every dimension for the Sphere function. This figure and the
    following ones have been generated with the BBOB report
    generator. The lines represent the average runtimes, the crosses
    the median runtime of successful runs to reach the most difficult
    target, x-crosses the max number of f-evaluations in any
    triel. The $y$ scale is logarithmic. The light line on the bottom
    with diammonds indicates the best algorithm from BBOB 2009. }
  % Does this means our runtime is worse than BBOB 2009? - JJ
  % Yes, at this moment but we have been working on the platform
  % not yet on the optimization. - M

  % Also I hope that this apprach will work better on more difficult
  % functions were more diversity is needed. - M

\label{fig:sphere}
\end{figure*}


\begin{figure*}[h!t]
    \centering
        \includegraphics[width=5in]{img/Ellipsoid.pdf}
    \caption{Average numbers of function evaluations to reach target
      for every dimension for the Ellipsoid function.}
    \label{fig:ellipsoid}
\end{figure*}

\begin{figure*}[h!t]
    \centering
        \includegraphics[width=5in]{img/Rastrigin.pdf}
    \caption{Average numbers of function evaluations to reach target
      for every dimension for the Rastrigin function.}
    \label{fig:rastrigin}
\end{figure*}


\section{Conclusions and Further Work}
\label{sec:conclusions}

% To be written

Future lines of work will focus on using other EA or 
meta-heuristic techniques, such as the Grey Wolf Optimizer 
or Differential Evolution for having workers that are 
heterogeneous in more than one sense. RPSS could be used 
in those cases where each algorithm has a different set of 
parameters, but also to randomly select the technique employed 
in each node. Another interesting line of work is the dynamic 
adaptation of parameters by measuring the diversity of each 
worker or returned sample. This adaptation could be especially 
useful in cases where the random parametrization technique 
seems to achieve bad results. 

\begin{acks}
This work has been supported in part by:  Ministerio espa\~{n}ol de
Econom\'{\i}a y Competitividad under project TIN2014-56494-C4-3-P
(UGR-EPHEMECH).
\end{acks}
