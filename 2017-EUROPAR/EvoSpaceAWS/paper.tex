
\documentclass{llncs}
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{subfigure}
\usepackage{url}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\sloppy

\title{Random Selection of Parameters in Asynchronous Pool-Based Evolutionary Algorithms}

\author{Mario Garc\'ia-Valdez\inst{1} 
\and Ren\'e M\'arquez\inst{1} \and Juan J. Merelo Guerv\'os\inst{2} \and  Leonardo Trujillo \inst{1}}

\institute{Instituto Tecnol\'ogico de Tijuana, Tijuana BC, Mexico
\and
Universidad de Granada, Granada, Spain
\email{mario@tectijuana.edu.mx}\\
\email{renemarquezvalenzuela@gmail.com}\\
\email{jmerelo@geneura.ugr.es}\\
\email{leonardo.trujillo@tectijuana.edu.mx}}

\maketitle

\begin{abstract}
It is not always possible to set up an distributed system with
homogeneous nodes to run algorithms in a synchronous way. In grid,
cloud or volunteer setups nodes are heterogeneous, or simply are not
available at the exact same time; this is a challenge for the researcher if
their full performance is going to be actually leveraged. We are
interested in evolutionary algorithms (EAs), which evolve a population
of solutions using a mechanism inspired by biological evolution; in this field, 
several asynchronous Evolutionary Algorithms (EAs) that distribute the
evolutionary process among heterogeneous nodes have
been proposed. These algorithms make the population shared between
distributed workers  which execute the actual evolutionary
process by taking samples of the population, and replacing them in the
population pool by evolved individuals. The performance of these EAs
depends in part on the selection of parameters for the EA running in
each worker, which may include sample size, generations, mutation rate
and crossover rate along with the overall configuration. In this paper
we present a method inspired by a strategy proposed by Gong and Fukunaga for the
Island-Model which statically assigns random parameter settings to
each island in a cloud setting. Experiments were conducted in the
cloud using 2, 6 and 12 virtual machine configurations, with
both homogeneous and heterogeneous random settings using five 
test functions for single-objective optimization (Rastrigin, Griewank, De Jong, Schaffer 
and Ackley) and the OneMax binary problem. The results suggest that this approach can yield
performance improvements which are competitive with instances of the
algorithm using workers with control parameters tuned specifically for
the benchmark.

\keywords{Distributed Evolutionary Algorithms, Volunteer Computing,
  Cloud Computing}
\end{abstract}
\section{Introduction}
%Pool Based
In this work, we are interested in EAs systems that follow a
pool-based approach, where the search process is conducted by a
collection of possibly heterogeneous processes that collaborate using
a shared repository or population pool. We will refer to such
algorithms as Pool-based EAs or PEAs, and highlight the fact that such
systems are intrinsically parallel, distributed and asynchronous. 

% EA Paramaters 
% Worst on many workers
% Explore and Exploit
% Dynamic adaptation
% Random Patameters
% Evaluate in EvoSpace

\section{Related Work}
% Dynamic adaptation
% Random Patameters

\section{EvoSpace Cloud Implementation}
% Architecture Components
% Random Patameters
\begin{figure*}[t]
    \centering
        \includegraphics[width=10cm]{img/evospace-aws.png}
    \caption{Main components and data-flow in the cloud version of EvoSpace. }
    \label{fig:evospace}
\end{figure*}

\section{Experimental Set-up}

The goal of this work is to determine if a random static parametrization for each of the $n$ EvoWorkers 
collaborating on a given run could achieve competitive results against an homogeneous tuned parametrization
\cite{fuku1,fuku2,garcia2014randomized}. The parameters considered to be randomly set for each EvoWorker 
where the crossover and mutation probabilities with a valid range of $[0,1]$. These parameters where 
selected because they effectively control which types of moves in the search space are
applied, and are related to the amount of exploration and explatation that is carried out \cite{}.
The other parameters and design choices used by EvoSpace and the EvoWorkers in our experiments are given in 
Table \ref{tab:params}. Additionally, for the binary OneMax problem bit-flip mutation
is used with an independent probability for each attribute to be flipped (indpb) of $0.05$, 
a two ponit crossover and a tournament selection of size 3. For the real-valued problems a Gaussian
mutation was applied with mu=$0$, sigma=$0.2$, and indpb=$0.05$; two point crossover and
a tournament selection of three. These values are kept static to measure only the effect of the 
parameters mentioned above.
    % Why do these parameters don't change?
    % Explained - Mario 


\begin{table}[!t]
\caption{Valid ranges for each EvoWorker parameters. One Max}
\label{tab:params}
\centering
\begin{tabular}{|l|c|c|c|c|c|c| }
\hline
\textbf{Parameter} & \multicolumn{3}{|c|}{OneMax} & \multicolumn{3}{|c|}{Test Functions} \\
\hline
Number of Workers & 2 & 6 & 12 & 2 & 6 & 12\\
\hline
\hline
Population Size & 200 & 280 & 620 & 120 & 150 & 150\\
\hline
Sample Size & 40 & 40 & 40 & 40 & 20 & 10\\
\hline
Maximum Samples & 30 & 40 & 40 & 200 & 100 & 50\\
\hline
Local Generations & 30 & 30 & 30 & 100 & 100 & 100\\
\hline
\end{tabular}
\end{table}

To measure the effectiveness of RPSS in EvoSpace two parametrization strategies are compared, 
similar to what is done in \cite{fuku1,fuku2,garcia2014randomized}. First, we consider the approach of setting all 
of the EvoWorker parameters homogeneously. In order to tune the homogeneous parameters,
we select the best configuration from 100 random parameterizations. 
Each of these random parametrizations are evaluated based on their average performance over 10 
independent runs for each problem.
Hereafter, we refer to this method as the {\em homogeneous} configuration. Second, for RPSS the parametrizations
are not tuned, they are randomly generated for each EvoWorker, set independently at the beginning of each run.
Hereafter, we call this approach the {\em heterogeneous} configuration. Finally, we perform 30 independent runs
with both methods and report the average number of evaluations required by each strategy to find the
global optimum solution.

The algorithms are first evaluated using the OneMax (or BitCounting) problem proposed by 
Schaffer and Eshelman \cite{SE91}, this is a simple problem consisting in maximizing the number 
of ones of a bitstring. For the current experiments a 128 bit string was used. Since this
problem is not computationally demanding, more experiments where conducted for the tuning phase, 
tuning the parameters using 2, 6, and 12 EvoWorkers. Then, following \cite{fuku1}, 
five standard real-valued single objective optimization problems 
are used, these are the Rastrigin, Griewank, De Jong, Schaffer  and Ackley functions, 
with the number of dimensions set to 100 in each case. For these problems we use a real-valued vector
representation for each individual.


\subsection{OneMax Results}

In this section, results from the OneMax experiments are discussed. As mentioned before, the first
step is to simulate the manual tuning of the parameters by running 100 experiments with random 
parameters and then select the best configuration. As the OneMax problem is solved by almost 
all configurations, the best is the one requiring the least time to finish. Figure~\ref{fig:effort} 
shows the time required for different numbers of EvoWorkers. It can be observed that even with an 
homogeneous configuration as the number of workers increases so does
the number of fast solutions.
A reason for this could be that the solution to the problem depends on the number of evaluations, and 
as the number of workers increases so does the number of evaluations
per unit of time, which implies
that the selection of a good configuration could be found with less experiments for higher number 
of workers.


In Figure~\ref{fig:comp-onemax} the results of 30 runs comparing
against the RPSS approach is shown. %What is RPSS? - JJ
        %It will be explained in the Intro is the Random Parameter Selection Strategy - Mario
It can be observed that as the number of EvoWorkers increases the median of the time decreases, but
in this case the best of the heterogeneous solutions (12 workers) is only better than the worst homogeneous
solution.  % Explain - JJ
           % To do - Mario 

\begin{figure*}[b]
    \centering
    \subfigure [2 workers]
    {
        \includegraphics[width=3in]{img/2w_onemax_100_box.png}
    }
    \subfigure  [6 workers]
    {
        \includegraphics[width=3in]{img/6w_onemax_100_box.png}
    }
    \subfigure  [12 workers]
    {
        \includegraphics[width=3in]{img/12w_onemax_100_box.png}
    }

    \caption{100 experiments with random parameters for the 128 Bit OneMax problem.
    Experiments are ranked by the mean time to solution of 5 runs, with   
    (a) 2 workers, (b) 6 workers, and (c) 12 workers.}
    \label{fig:effort}
\end{figure*}



\begin{figure*}[t]
    \centering
        \includegraphics[width=10cm]{img/one_max_comp.png}
    \caption{Comparison of 30 runs of the 128 Bit OneMax problem. 
    Box-plot of the number of evaluations needed for solution, with a 2, 6 and 12 workers
    homogeneous configuration on the left side, and Heterogeneous configuration on the
    right side of each.
    }
    \label{fig:comp-onemax}
\end{figure*}



\subsection{ Single-Objective Optimization Test Functions}

The same steps as the previous section were followed when comparing the five real valued 
optimization problems of this section. For brevity only two representative functions are discussed
with figures, and summary of all the results are shown in Table~\ref{fig:summary}. As before
the first step is to tune the parameters of the 
asynchronous GA algorithm. In Figure \ref{fig:griewank} the results of the tuning 
phase for 6 and 12 workers for the Griewank function are shown. For 12 workers
results are not as flat as before, with times comparable to the 6 worker 
configuration in the best case.

In order to compare both configurations in Figure \ref{fig:griewank-evals} the number of evaluations 
needed for solution are presented. In this experiment, the heterogeneous solution is better than the
tuned homogeneous parametrization. But in the case of results shown in Figure~\ref{fig:schaffer} 
the homogeneous and heterogeneous configurations yield similar results, 
only with a slight advantage
for the heterogeneous configuration with 12 EvoWorkers. In the summary shown in
Table~\ref{fig:summary} two extreme cases are found the Ackley and Rastrigin functions, 
with results leaning
towards the heterogeneous and homogeneous configurations respectively. On the other hand, the De Jong
function also favors the heterogeneous configurations. 



\begin{figure*}[b]
    \centering
    \subfigure  [6 workers]
    {
        \includegraphics[width=2.2in]{img/6w_griewank_100_box.png}
    }
    \subfigure  [12 workers]
    {
        \includegraphics[width=2.2in]{img/12w_griewank_100_box.png}
    }

    \caption{100 experiments with random parameters for the 128 Bit Griewank 
    single-objective optimization test function. Experiments are ranked by 
    the mean time to solution of 5 runs, with (a) 6 workers, and (b) 12 workers.}
    \label{fig:griewank}
\end{figure*}


\begin{figure*}[b]
    \centering
    \subfigure  [Homogeneous]
    {
        \includegraphics[width=2.2in]{img/griewank_evals_homo.png}
    }
    \subfigure  [Heterogeneous]
    {
        \includegraphics[width=2.2in]{img/griewank_evals_hetereo.png}
    }
      \caption{30 runs of the 128 dimension Griewank single-objective optimization test function. 
    Box-plot of the number of evaluations needed for solution, with an (a) Homogeneous configuration, and (b) Heterogeneous configuration.}
    \label{fig:griewank-evals}
\end{figure*}

\begin{figure*}[b]
    \centering
    \subfigure  [Homogeneous]
    {
        \includegraphics[width=2.2in]{img/schaffer_evals_homo.png}
    }
    \subfigure  [Heterogeneous]
    {
        \includegraphics[width=2.2in]{img/schaffer_evals_hetereo.png}
    }
 

    \caption{30 runs of the 128 dimension Schaffer single-objective optimization test function. 
    Box-plot of the number of evaluations needed for solution, with an (a) Homogeneous configuration, and (b) Heterogeneous configuration.}
    \label{fig:schaffer}
\end{figure*}

\begin{figure*}[t]
    \centering
        \includegraphics[width=10cm]{img/table.png}
    \caption{Summary table of results. }
    \label{fig:summary}
\end{figure*}

\bibliographystyle{abbrv}
\bibliography{../../bib/biblio}

\end{document}
